---
title: æ„å»ºæ™ºèƒ½AIåº”ç”¨ï¼šä»LLMåˆ°AI Agentçš„å®Œæ•´æŒ‡å—
slug: build-ai-applications-from-llm-to-agent
date: 2026-01-21 11:09:10
categories:
  - å‰ç«¯
  - AI
tags:
  - AI
  - LLM
  - AI Agent
  - LangChain
  - å®æˆ˜æ•™ç¨‹
index_img: https://images.unsplash.com/photo-1655720828018-edd2daec9349?w=1920&h=400&fit=crop
cover: /images/Building Intelligent AI Applications.webp
top_img: /images/Building Intelligent AI Applications.webp
banner_img: https://images.unsplash.com/photo-1655720828018-edd2daec9349?w=1920&h=400&fit=crop
---

## å‰è¨€

![AI Neural Network](https://images.unsplash.com/photo-1677442136019-21780ecad995?w=1200&h=600&fit=crop)

éšç€ GPT-4ã€Claudeã€Gemini ç­‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çˆ†å‘å¼å‘å±•ï¼ŒAI åº”ç”¨å¼€å‘å·²ç»è¿›å…¥äº†ä¸€ä¸ªå…¨æ–°çš„æ—¶ä»£ã€‚ä»ç®€å•çš„èŠå¤©æœºå™¨äººåˆ°å¤æ‚çš„ AI Agentï¼Œä»å•ä¸€æ¨¡å‹è°ƒç”¨åˆ° RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿï¼Œå¼€å‘è€…æ­£åœ¨æ„å»ºè¶Šæ¥è¶Šæ™ºèƒ½çš„åº”ç”¨ã€‚

æœ¬æ–‡å°†å¸¦ä½ æ·±å…¥äº†è§£å¦‚ä½•æ„å»ºç°ä»£ AI åº”ç”¨ï¼Œæ¶µç›– LLM é›†æˆã€Prompt Engineeringã€AI Agent å¼€å‘å’Œ RAG ç³»ç»Ÿå®æˆ˜ã€‚

## ä¸€ã€LLM åŸºç¡€ä¸ API é›†æˆ

### 1.1 ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰æ˜¯åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œé€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œå­¦ä¼šäº†ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

**ä¸»æµ LLM å¯¹æ¯”ï¼š**

| æ¨¡å‹ | æä¾›å•† | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|----------|
| GPT-4 | OpenAI | ç»¼åˆèƒ½åŠ›å¼ºï¼Œä¸Šä¸‹æ–‡é•¿ | é€šç”¨ä»»åŠ¡ã€å¤æ‚æ¨ç† |
| Claude 3 | Anthropic | å®‰å…¨æ€§å¼ºï¼Œé•¿æ–‡æœ¬ | åˆ†æã€æ€»ç»“ã€ä»£ç  |
| Gemini | Google | å¤šæ¨¡æ€èƒ½åŠ›å¼º | å›¾æ–‡ç†è§£ã€è§†é¢‘åˆ†æ |
| Llama 3 | Meta | å¼€æºï¼Œå¯æœ¬åœ°éƒ¨ç½² | éšç§è¦æ±‚é«˜çš„åœºæ™¯ |

### 1.2 å¿«é€Ÿå¼€å§‹ï¼šOpenAI API é›†æˆ

é¦–å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
npm install openai dotenv
```

åˆ›å»ºåŸºç¡€çš„ LLM è°ƒç”¨ç±»ï¼š

```typescript
// src/llm/openai-client.ts
import OpenAI from 'openai';

export class LLMClient {
  private client: OpenAI;

  constructor(apiKey: string) {
    this.client = new OpenAI({
      apiKey: apiKey,
      dangerouslyAllowBrowser: false // ç”Ÿäº§ç¯å¢ƒå¿…é¡»é€šè¿‡åç«¯è°ƒç”¨
    });
  }

  async chat(
    messages: Array<{role: 'user' | 'assistant' | 'system'; content: string}>,
    model: string = 'gpt-4'
  ): Promise<string> {
    try {
      const response = await this.client.chat.completions.create({
        model: model,
        messages: messages,
        temperature: 0.7, // æ§åˆ¶éšæœºæ€§ï¼Œ0-2ï¼Œè¶Šé«˜è¶Šéšæœº
        max_tokens: 2000,  // æœ€å¤§è¾“å‡º token æ•°
      });

      return response.choices[0]?.message?.content || '';
    } catch (error) {
      console.error('LLM API Error:', error);
      throw error;
    }
  }

  // æµå¼è¾“å‡ºï¼Œé€‚åˆå®æ—¶å“åº”
  async *chatStream(
    messages: Array<{role: string; content: string}>,
    model: string = 'gpt-4'
  ): AsyncGenerator<string, void, unknown> {
    const stream = await this.client.chat.completions.create({
      model: model,
      messages: messages,
      stream: true,
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        yield content;
      }
    }
  }
}
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```typescript
// ç¤ºä¾‹ï¼šåŸºç¡€å¯¹è¯
const llm = new LLMClient(process.env.OPENAI_API_KEY!);

const messages = [
  {
    role: 'system',
    content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®ï¼Œæ“…é•¿å¸®åŠ©å¼€å‘è€…è§£å†³é—®é¢˜ã€‚'
  },
  {
    role: 'user',
    content: 'è¯·è§£é‡Šä»€ä¹ˆæ˜¯ React Hooksï¼Ÿ'
  }
];

const response = await llm.chat(messages);
console.log(response);
```

### 1.3 å®‰å…¨çš„ API è°ƒç”¨ï¼šåç«¯å°è£…

**é‡è¦**ï¼šæ°¸è¿œä¸è¦åœ¨å‰ç«¯ç›´æ¥è°ƒç”¨ LLM APIï¼Œè¿™ä¼šæš´éœ²ä½ çš„ API Keyï¼

åˆ›å»º Next.js API è·¯ç”±ï¼š

```typescript
// pages/api/chat.ts
import type { NextApiRequest, NextApiResponse } from 'next';
import { LLMClient } from '@/lib/llm/openai-client';

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  const { messages } = req.body;

  // éªŒè¯è¯·æ±‚
  if (!messages || !Array.isArray(messages)) {
    return res.status(400).json({ error: 'Invalid messages' });
  }

  try {
    const llm = new LLMClient(process.env.OPENAI_API_KEY!);
    const response = await llm.chat(messages);

    res.status(200).json({ response });
  } catch (error) {
    console.error('Chat API Error:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
}
```

å‰ç«¯è°ƒç”¨ï¼š

```typescript
// components/ChatInterface.tsx
import { useState } from 'react';

export function ChatInterface() {
  const [messages, setMessages] = useState<Array<{
    role: 'user' | 'assistant';
    content: string;
  }>>([]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);

  const sendMessage = async () => {
    if (!input.trim()) return;

    const userMessage = { role: 'user' as const, content: input };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setLoading(true);

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ messages: [...messages, userMessage] })
      });

      const data = await response.json();
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: data.response
      }]);
    } catch (error) {
      console.error('Error:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="chat-container">
      <div className="messages">
        {messages.map((msg, idx) => (
          <div key={idx} className={`message ${msg.role}`}>
            {msg.content}
          </div>
        ))}
        {loading && <div className="message assistant">Thinking...</div>}
      </div>

      <div className="input-area">
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
          placeholder="è¾“å…¥ä½ çš„é—®é¢˜..."
        />
        <button onClick={sendMessage} disabled={loading}>
          å‘é€
        </button>
      </div>
    </div>
  );
}
```

## äºŒã€Prompt Engineering æœ€ä½³å®è·µ

### 2.1 ä»€ä¹ˆæ˜¯ Prompt Engineeringï¼Ÿ

Prompt Engineeringï¼ˆæç¤ºå·¥ç¨‹ï¼‰æ˜¯è®¾è®¡å’Œä¼˜åŒ–è¾“å…¥ç»™ LLM çš„æç¤ºè¯ï¼Œä»¥è·å¾—æ›´å‡†ç¡®ã€æ›´ç›¸å…³è¾“å‡ºçš„æŠ€æœ¯ã€‚

### 2.2 æ ¸å¿ƒåŸåˆ™

**1. æ¸…æ™°å…·ä½“**

```typescript
// âŒ ä¸å¥½çš„ prompt
const badPrompt = "å†™ä»£ç ";

// âœ… å¥½çš„ prompt
const goodPrompt = `
è¯·ç”¨ TypeScript å†™ä¸€ä¸ª React å‡½æ•°ç»„ä»¶ï¼Œå®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
1. æ˜¾ç¤ºä¸€ä¸ªè®¡æ•°å™¨
2. æœ‰å¢åŠ å’Œå‡å°‘æŒ‰é’®
3. ä½¿ç”¨ useState hook
4. æ·»åŠ é€‚å½“çš„ç±»å‹æ³¨è§£
5. åŒ…å«åŸºæœ¬çš„æ ·å¼
`;
```

**2. æä¾›ä¸Šä¸‹æ–‡**

```typescript
const systemPrompt = `
ä½ æ˜¯ä¸€ä½æœ‰ 10 å¹´ç»éªŒçš„å‰ç«¯å·¥ç¨‹å¸ˆï¼Œä¸“ç²¾äº React å’Œ TypeScriptã€‚
ä½ çš„å›ç­”åº”è¯¥ï¼š
- æŠ€æœ¯å‡†ç¡®ï¼Œéµå¾ªæœ€ä½³å®è·µ
- ä»£ç ç®€æ´ï¼Œæ˜“äºç»´æŠ¤
- åŒ…å«å¿…è¦çš„æ³¨é‡Š
- è€ƒè™‘æ€§èƒ½å’Œå¯è®¿é—®æ€§
`;
```

**3. ä½¿ç”¨ç¤ºä¾‹ï¼ˆFew-shot Learningï¼‰**

```typescript
const prompt = `
ä»»åŠ¡ï¼šå°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸º SQL æŸ¥è¯¢

ç¤ºä¾‹ 1ï¼š
è¾“å…¥ï¼šæŸ¥æ‰¾æ‰€æœ‰å¹´é¾„å¤§äº 25 çš„ç”¨æˆ·
è¾“å‡ºï¼šSELECT * FROM users WHERE age > 25;

ç¤ºä¾‹ 2ï¼š
è¾“å…¥ï¼šè·å–é”€é‡å‰ 10 çš„äº§å“
è¾“å‡ºï¼šSELECT * FROM products ORDER BY sales DESC LIMIT 10;

è¾“å…¥ï¼šæŸ¥æ‰¾åå­—åŒ…å« "John" çš„ç”¨æˆ·
è¾“å‡ºï¼š
`;
```

**4. é“¾å¼æ€è€ƒï¼ˆChain of Thoughtï¼‰**

```typescript
const prompt = `
é—®é¢˜ï¼šå°æ˜æœ‰ 5 ä¸ªè‹¹æœï¼Œä»–ç»™äº†å°çº¢ 2 ä¸ªï¼Œç„¶ååˆä¹°äº† 3 ä¸ªï¼Œç°åœ¨ä»–æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ

è¯·ä¸€æ­¥æ­¥æ€è€ƒï¼š
1. åˆå§‹æ•°é‡ï¼šå°æ˜æœ‰ 5 ä¸ªè‹¹æœ
2. ç»™å°çº¢åï¼š5 - 2 = 3 ä¸ª
3. ä¹°å…¥åï¼š3 + 3 = 6 ä¸ª

ç­”æ¡ˆï¼šå°æ˜ç°åœ¨æœ‰ 6 ä¸ªè‹¹æœã€‚

ç°åœ¨è¯·ç”¨åŒæ ·çš„æ–¹å¼è§£å†³ï¼š
å°çº¢æœ‰ 10 ä¸ªæ©™å­ï¼Œå¥¹ç»™å°ä¸½ 3 ä¸ªï¼Œåˆåƒäº† 2 ä¸ªï¼Œç„¶åå¦ˆå¦ˆç»™äº†å¥¹ 5 ä¸ªï¼Œç°åœ¨å¥¹æœ‰å¤šå°‘ä¸ªæ©™å­ï¼Ÿ
`;
```

### 2.3 ç»“æ„åŒ– Prompt æ¨¡æ¿

```typescript
// src/prompts/structured-prompt.ts
export interface PromptTemplate {
  role: 'system' | 'user' | 'assistant';
  template: string;
  variables?: Record<string, any>;
}

export class PromptBuilder {
  private messages: Array<{role: string; content: string}> = [];

  addSystem(content: string): this {
    this.messages.push({ role: 'system', content });
    return this;
  }

  addUser(content: string, variables?: Record<string, any>): this {
    let formattedContent = content;
    if (variables) {
      Object.entries(variables).forEach(([key, value]) => {
        formattedContent = formattedContent.replace(
          new RegExp(`\\{\\{${key}\\}\\}`, 'g'),
          String(value)
        );
      });
    }
    this.messages.push({ role: 'user', content: formattedContent });
    return this;
  }

  build(): Array<{role: string; content: string}> {
    return [...this.messages];
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const prompt = new PromptBuilder()
  .addSystem('ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥å‘˜')
  .addUser(`
    è¯·å®¡æŸ¥ä»¥ä¸‹ {{language}} ä»£ç ï¼Œé‡ç‚¹å…³æ³¨ï¼š
    1. ä»£ç è´¨é‡
    2. æ½œåœ¨çš„ bug
    3. æ€§èƒ½é—®é¢˜
    4. æ”¹è¿›å»ºè®®

    ä»£ç ï¼š
    ```{{language}}
    {{code}}
    ```
  `, {
    language: 'TypeScript',
    code: 'const data = await fetch(url);'
  })
  .build();
```

## ä¸‰ã€æ„å»º AI Agent

### 3.1 ä»€ä¹ˆæ˜¯ AI Agentï¼Ÿ

AI Agent æ˜¯èƒ½å¤Ÿè‡ªä¸»æ„ŸçŸ¥ç¯å¢ƒã€åšå‡ºå†³ç­–å¹¶æ‰§è¡Œè¡ŒåŠ¨çš„ AI ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿ LLM èŠå¤©æœºå™¨äººä¸åŒï¼ŒAgent å¯ä»¥ï¼š

- ğŸ”§ ä½¿ç”¨å·¥å…·ï¼ˆToolsï¼‰
- ğŸ”„ å¤šæ­¥æ¨ç†
- ğŸ“Š è®¿é—®å¤–éƒ¨æ•°æ®
- ğŸ¤ ä¸å…¶ä»– Agent åä½œ

![AI Agent Architecture](https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=1200&h=600&fit=crop)

### 3.2 Agent æ ¸å¿ƒç»„ä»¶

```typescript
// src/agent/types.ts
export interface Tool {
  name: string;
  description: string;
  parameters: Record<string, any>;
  execute: (params: any) => Promise<any>;
}

export interface AgentConfig {
  name: string;
  role: string;
  tools: Tool[];
  llmClient: LLMClient;
}

export interface Task {
  id: string;
  description: string;
  status: 'pending' | 'in_progress' | 'completed' | 'failed';
  result?: any;
  error?: string;
}
```

### 3.3 å®ç°åŸºç¡€ Agent

```typescript
// src/agent/base-agent.ts
import { LLMClient } from '../llm/openai-client';

export class Agent {
  protected llm: LLMClient;
  protected tools: Map<string, Tool> = new Map();
  protected memory: Array<{role: string; content: string}> = [];

  constructor(llm: LLMClient) {
    this.llm = llm;
  }

  registerTool(tool: Tool): void {
    this.tools.set(tool.name, tool);
  }

  async think(userInput: string): Promise<string> {
    // 1. åˆ†æç”¨æˆ·æ„å›¾
    const intent = await this.analyzeIntent(userInput);

    // 2. å†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
    if (intent.requiresTool) {
      const toolResult = await this.executeTool(intent.toolName, intent.parameters);
      return toolResult;
    }

    // 3. ç›´æ¥å›ç­”
    const response = await this.llm.chat([
      { role: 'system', content: this.getSystemPrompt() },
      ...this.memory,
      { role: 'user', content: userInput }
    ]);

    // ä¿å­˜åˆ°è®°å¿†
    this.memory.push({ role: 'user', content: userInput });
    this.memory.push({ role: 'assistant', content: response });

    return response;
  }

  private async analyzeIntent(userInput: string): Promise<{
    requiresTool: boolean;
    toolName?: string;
    parameters?: any;
  }> {
    const toolsDescription = Array.from(this.tools.values())
      .map(tool => `- ${tool.name}: ${tool.description}`)
      .join('\n');

    const prompt = `
      å¯ç”¨å·¥å…·ï¼š
      ${toolsDescription}

      ç”¨æˆ·è¾“å…¥ï¼š${userInput}

      åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ã€‚å¦‚æœéœ€è¦ï¼Œè¿”å› JSON æ ¼å¼ï¼š
      {
        "requiresTool": true,
        "toolName": "å·¥å…·åç§°",
        "parameters": {å‚æ•°}
      }

      å¦‚æœä¸éœ€è¦ï¼Œè¿”å›ï¼š
      {
        "requiresTool": false
      }
    `;

    const response = await this.llm.chat([
      { role: 'user', content: prompt }
    ]);

    return JSON.parse(response);
  }

  private async executeTool(toolName: string, parameters: any): Promise<string> {
    const tool = this.tools.get(toolName);
    if (!tool) {
      return `å·¥å…· ${toolName} ä¸å­˜åœ¨`;
    }

    try {
      const result = await tool.execute(parameters);
      return `å·¥å…·æ‰§è¡Œç»“æœï¼š${JSON.stringify(result, null, 2)}`;
    } catch (error) {
      return `å·¥å…·æ‰§è¡Œå¤±è´¥ï¼š${error}`;
    }
  }

  protected getSystemPrompt(): string {
    return 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚';
  }
}
```

### 3.4 å®ç”¨å·¥å…·ç¤ºä¾‹

**1. ç½‘é¡µæœç´¢å·¥å…·**

```typescript
// src/agent/tools/search.ts
import axios from 'axios';

export const searchTool: Tool = {
  name: 'web_search',
  description: 'åœ¨äº’è”ç½‘ä¸Šæœç´¢ä¿¡æ¯',
  parameters: {
    query: { type: 'string', description: 'æœç´¢å…³é”®è¯' },
    numResults: { type: 'number', description: 'ç»“æœæ•°é‡', default: 5 }
  },
  execute: async (params) => {
    // ä½¿ç”¨ Google Custom Search API æˆ–å…¶ä»–æœç´¢ API
    const response = await axios.get('https://serpapi.com/search', {
      params: {
        q: params.query,
        api_key: process.env.SERPAPI_KEY
      }
    });

    return {
      results: response.data.organic_results?.slice(0, params.numResults || 5)
    };
  }
};
```

**2. ä»£ç æ‰§è¡Œå·¥å…·**

```typescript
// src/agent/tools/code-executor.ts
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);

export const codeExecutorTool: Tool = {
  name: 'execute_code',
  description: 'æ‰§è¡Œ JavaScript/TypeScript ä»£ç å¹¶è¿”å›ç»“æœ',
  parameters: {
    code: { type: 'string', description: 'è¦æ‰§è¡Œçš„ä»£ç ' },
    language: { type: 'string', description: 'ä»£ç è¯­è¨€', default: 'javascript' }
  },
  execute: async (params) => {
    try {
      // æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒéœ€è¦æ²™ç®±ç¯å¢ƒï¼Œè¿™é‡Œä»…ä½œæ¼”ç¤º
      const result = await execAsync(`node -e "${params.code}"`);
      return {
        success: true,
        output: result.stdout,
        error: result.stderr
      };
    } catch (error) {
      return {
        success: false,
        error: error
      };
    }
  }
};
```

**3. æ–‡ä»¶æ“ä½œå·¥å…·**

```typescript
// src/agent/tools/file-operations.ts
import fs from 'fs/promises';
import path from 'path';

export const fileReadTool: Tool = {
  name: 'read_file',
  description: 'è¯»å–æ–‡ä»¶å†…å®¹',
  parameters: {
    filePath: { type: 'string', description: 'æ–‡ä»¶è·¯å¾„' }
  },
  execute: async (params) => {
    try {
      const content = await fs.readFile(params.filePath, 'utf-8');
      return { success: true, content };
    } catch (error) {
      return { success: false, error: (error as Error).message };
    }
  }
};

export const fileWriteTool: Tool = {
  name: 'write_file',
  description: 'å†™å…¥æ–‡ä»¶å†…å®¹',
  parameters: {
    filePath: { type: 'string', description: 'æ–‡ä»¶è·¯å¾„' },
    content: { type: 'string', description: 'æ–‡ä»¶å†…å®¹' }
  },
  execute: async (params) => {
    try {
      await fs.writeFile(params.filePath, params.content, 'utf-8');
      return { success: true };
    } catch (error) {
      return { success: false, error: (error as Error).message };
    }
  }
};
```

### 3.5 åˆ›å»ºä¸“ç”¨ Agent

```typescript
// src/agents/code-review-agent.ts
import { Agent } from './base-agent';
import { codeExecutorTool } from '../agent/tools/code-executor';

export class CodeReviewAgent extends Agent {
  constructor(llm: LLMClient) {
    super(llm);
    this.registerTool(codeExecutorTool);
  }

  protected getSystemPrompt(): string {
    return `
      ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»£ç å®¡æŸ¥å‘˜ï¼Œä¸“ç²¾äºå¤šç§ç¼–ç¨‹è¯­è¨€ã€‚

      å®¡æŸ¥ä»£ç æ—¶ï¼Œä½ å…³æ³¨ï¼š
      1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§
      2. æ½œåœ¨çš„ bug å’Œè¾¹ç•Œæƒ…å†µ
      3. æ€§èƒ½ä¼˜åŒ–æœºä¼š
      4. å®‰å…¨æ¼æ´
      5. æœ€ä½³å®è·µéµå¾ªæƒ…å†µ

      ä½ çš„å›å¤åº”è¯¥ï¼š
      - å»ºè®¾æ€§ä¸”å…·ä½“
      - æä¾›æ”¹è¿›å»ºè®®å’Œä»£ç ç¤ºä¾‹
      - è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·æ”¹è¿›
    `;
  }

  async reviewCode(code: string, language: string): Promise<{
    summary: string;
    issues: Array<{
      severity: 'high' | 'medium' | 'low';
      line?: number;
      description: string;
      suggestion?: string;
    }>;
    overallScore: number;
  }> {
    const prompt = `
      è¯·å®¡æŸ¥ä»¥ä¸‹ ${language} ä»£ç ï¼š

      \`\`\`${language}
      ${code}
      \`\`\`

      æä¾›è¯¦ç»†çš„å®¡æŸ¥æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
      1. æ€»ä½“è¯„ä»·ï¼ˆ1-10åˆ†ï¼‰
      2. å‘ç°çš„é—®é¢˜ï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼‰
      3. æ¯ä¸ªé—®é¢˜çš„å…·ä½“ä½ç½®å’Œæ”¹è¿›å»ºè®®
    `;

    const response = await this.llm.chat([
      { role: 'system', content: this.getSystemPrompt() },
      { role: 'user', content: prompt }
    ]);

    // è§£æç»“æ„åŒ–å“åº”ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥è®© LLM è¿”å› JSONï¼‰
    return {
      summary: response,
      issues: [],
      overallScore: 8
    };
  }
}
```

## å››ã€RAG ç³»ç»Ÿå®æˆ˜

### 4.1 ä»€ä¹ˆæ˜¯ RAGï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é€šè¿‡ç»“åˆä¿¡æ¯æ£€ç´¢å’Œ LLM ç”Ÿæˆï¼Œè®© AI èƒ½å¤Ÿè®¿é—®å’Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“ï¼Œå¤§å¤§æå‡äº†å›ç­”çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚

![RAG Architecture](https://images.unsplash.com/photo-1550751827-4bd374c3f58b?w=1200&h=600&fit=crop)

**RAG çš„ä¼˜åŠ¿ï¼š**
- âœ… å‡å°‘å¹»è§‰ï¼ˆHallucinationï¼‰
- âœ… çŸ¥è¯†å®æ—¶æ›´æ–°
- âœ… å¯æº¯æºå¼•ç”¨
- âœ… é¢†åŸŸä¸“ä¸šçŸ¥è¯†

### 4.2 RAG ç³»ç»Ÿæ¶æ„

```
ç”¨æˆ·æŸ¥è¯¢
    â†“
å‘é‡åŒ– Embedding
    â†“
å‘é‡æ£€ç´¢ â† â†’ çŸ¥è¯†åº“ï¼ˆå‘é‡æ•°æ®åº“ï¼‰
    â†“
ä¸Šä¸‹æ–‡æ„å»º
    â†“
LLM ç”Ÿæˆ
    â†“
ç­”æ¡ˆ + å¼•ç”¨
```

### 4.3 å®ç°å‘é‡å­˜å‚¨

é¦–å…ˆå®‰è£…ä¾èµ–ï¼š

```bash
npm install @langchain/openai @langchain/pinecone pinecone-client
```

åˆ›å»ºå‘é‡å­˜å‚¨æœåŠ¡ï¼š

```typescript
// src/rag/vector-store.ts
import { PineconeClient } from '@pinecone-database/pinecone';
import { OpenAIEmbeddings } from '@langchain/openai';

export class VectorStore {
  private pinecone: PineconeClient;
  private embeddings: OpenAIEmbeddings;
  private index: any;

  constructor() {
    this.pinecone = new PineconeClient({
      apiKey: process.env.PINECONE_API_KEY!,
      environment: process.env.PINECONE_ENVIRONMENT!
    });

    this.embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY
    });
  }

  async initialize(indexName: string) {
    await this.pinecone.init({
      apiKey: process.env.PINECONE_API_KEY!,
      environment: process.env.PINECONE_ENVIRONMENT!
    });

    this.index = this.pinecone.Index(indexName);
  }

  async addDocuments(
    documents: Array<{
      id: string;
      text: string;
      metadata?: Record<string, any>;
    }>
  ) {
    const vectors = await Promise.all(
      documents.map(async (doc) => {
        const embedding = await this.embeddings.embedQuery(doc.text);

        return {
          id: doc.id,
          values: embedding,
          metadata: {
            ...doc.metadata,
            text: doc.text
          }
        };
      })
    );

    await this.index.upsert({
      vectors: vectors,
      namespace: 'default'
    });
  }

  async similaritySearch(
    query: string,
    topK: number = 5,
    filter?: Record<string, any>
  ): Promise<Array<{id: string; score: number; metadata: any}>> {
    const queryEmbedding = await this.embeddings.embedQuery(query);

    const result = await this.index.query({
      queryRequest: {
        vector: queryEmbedding,
        topK: topK,
        includeMetadata: true,
        filter: filter
      }
    });

    return result.matches?.map((match: any) => ({
      id: match.id,
      score: match.score,
      metadata: match.metadata
    })) || [];
  }
}
```

### 4.4 æ–‡æ¡£å¤„ç†ä¸åˆ‡åˆ†

```typescript
// src/rag/document-loader.ts
export class DocumentLoader {
  /**
   * æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†æ–‡æ¡£
   */
  static splitByCharacter(
    text: string,
    chunkSize: number = 1000,
    overlap: number = 200
  ): string[] {
    const chunks: string[] = [];
    let start = 0;

    while (start < text.length) {
      const end = start + chunkSize;
      const chunk = text.slice(start, end);
      chunks.push(chunk);
      start = end - overlap;
    }

    return chunks;
  }

  /**
   * æŒ‰æ®µè½åˆ‡åˆ†ï¼ˆä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼‰
   */
  static splitByParagraph(text: string): string[] {
    return text
      .split(/\n\n+/)
      .map(p => p.trim())
      .filter(p => p.length > 0);
  }

  /**
   * æŒ‰æ ‡é¢˜å±‚æ¬¡åˆ‡åˆ†ï¼ˆMarkdown ä¸“ç”¨ï¼‰
   */
  static splitMarkdownByHeading(text: string): Array<{
    title: string;
    level: number;
    content: string;
  }> {
    const sections: Array<{title: string; level: number; content: string}> = [];
    const lines = text.split('\n');
    let currentSection = { title: 'Introduction', level: 0, content: '' };

    for (const line of lines) {
      const headingMatch = line.match(/^(#{1,6})\s+(.+)$/);

      if (headingMatch) {
        // ä¿å­˜å½“å‰ç« èŠ‚
        if (currentSection.content) {
          sections.push({...currentSection});
        }

        // å¼€å§‹æ–°ç« èŠ‚
        currentSection = {
          title: headingMatch[2],
          level: headingMatch[1].length,
          content: ''
        };
      } else {
        currentSection.content += line + '\n';
      }
    }

    if (currentSection.content) {
      sections.push(currentSection);
    }

    return sections;
  }

  /**
   * æ™ºèƒ½åˆ‡åˆ†ï¼šç»“åˆå¤šç§ç­–ç•¥
   */
  static async smartSplit(
    text: string,
    options: {
      maxSize?: number;
      overlap?: number;
      preserveStructure?: boolean;
    } = {}
  ): Promise<Array<{id: string; text: string; metadata: any}>> {
    const chunks: Array<{id: string; text: string; metadata: any}> = [];
    const { maxSize = 1000, overlap = 200, preserveStructure = true } = options;

    if (preserveStructure) {
      // ä¼˜å…ˆæŒ‰æ®µè½åˆ‡åˆ†
      const paragraphs = this.splitByParagraph(text);

      for (let i = 0; i < paragraphs.length; i++) {
        const paragraph = paragraphs[i];

        // å¦‚æœæ®µè½å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ‡åˆ†
        if (paragraph.length > maxSize) {
          const subChunks = this.splitByCharacter(paragraph, maxSize, overlap);
          subChunks.forEach((chunk, j) => {
            chunks.push({
              id: `chunk_${i}_${j}`,
              text: chunk,
              metadata: {
                source: 'paragraph',
                paragraphIndex: i,
                chunkIndex: j
              }
            });
          });
        } else {
          chunks.push({
            id: `paragraph_${i}`,
            text: paragraph,
            metadata: {
              source: 'paragraph',
              index: i
            }
          });
        }
      }
    } else {
      // ç›´æ¥æŒ‰å­—ç¬¦åˆ‡åˆ†
      const textChunks = this.splitByCharacter(text, maxSize, overlap);
      textChunks.forEach((chunk, i) => {
        chunks.push({
          id: `chunk_${i}`,
          text: chunk,
          metadata: { index: i }
        });
      });
    }

    return chunks;
  }
}
```

### 4.5 å®Œæ•´çš„ RAG ç®¡é“

```typescript
// src/rag/rag-pipeline.ts
import { VectorStore } from './vector-store';
import { DocumentLoader } from './document-loader';
import { LLMClient } from '../llm/openai-client';

export class RAGPipeline {
  private vectorStore: VectorStore;
  private llm: LLMClient;

  constructor(vectorStore: VectorStore, llm: LLMClient) {
    this.vectorStore = vectorStore;
    this.llm = llm;
  }

  /**
   * ç´¢å¼•æ–‡æ¡£
   */
  async indexDocuments(
    documents: Array<{
      id: string;
      text: string;
      metadata?: Record<string, any>;
    }>
  ) {
    const chunks = await Promise.all(
      documents.map(async (doc) => {
        const splitChunks = await DocumentLoader.smartSplit(doc.text, {
          maxSize: 500,
          overlap: 50,
          preserveStructure: true
        });

        return splitChunks.map((chunk, idx) => ({
          id: `${doc.id}_${idx}`,
          text: chunk.text,
          metadata: {
            ...doc.metadata,
            ...chunk.metadata,
            docId: doc.id
          }
        }));
      })
    );

    const allChunks = chunks.flat();
    await this.vectorStore.addDocuments(allChunks);

    console.log(`Indexed ${allChunks.length} chunks from ${documents.length} documents`);
  }

  /**
   * æŸ¥è¯¢å¹¶ç”Ÿæˆç­”æ¡ˆ
   */
  async query(
    question: string,
    options: {
      topK?: number;
      filter?: Record<string, any>;
      includeSources?: boolean;
    } = {}
  ): Promise<{
    answer: string;
    sources?: Array<{
      id: string;
      text: string;
      score: number;
      metadata: any;
    }>;
  }> {
    const { topK = 5, filter, includeSources = true } = options;

    // 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    const relevantDocs = await this.vectorStore.similaritySearch(
      question,
      topK,
      filter
    );

    // 2. æ„å»ºä¸Šä¸‹æ–‡
    const context = relevantDocs
      .map((doc, idx) => `[æ¥æº ${idx + 1}] ${doc.metadata.text}`)
      .join('\n\n');

    // 3. ç”Ÿæˆæç¤º
    const prompt = `
      åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

      ä¸Šä¸‹æ–‡ï¼š
      ${context}

      é—®é¢˜ï¼š${question}

      ç­”æ¡ˆï¼š
    `;

    // 4. ç”Ÿæˆç­”æ¡ˆ
    const answer = await this.llm.chat([
      {
        role: 'system',
        content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè´Ÿè´£åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å‡†ç¡®å›ç­”é—®é¢˜ã€‚'
      },
      { role: 'user', content: prompt }
    ]);

    // 5. è¿”å›ç»“æœ
    const result: any = { answer };

    if (includeSources) {
      result.sources = relevantDocs.map(doc => ({
        id: doc.id,
        text: doc.metadata.text?.substring(0, 200) + '...',
        score: doc.score,
        metadata: doc.metadata
      }));
    }

    return result;
  }
}
```

### 4.6 ä½¿ç”¨ RAG æ„å»ºçŸ¥è¯†é—®ç­”ç³»ç»Ÿ

```typescript
// src/rag/knowledge-base.ts
import { RAGPipeline } from './rag-pipeline';
import { VectorStore } from './vector-store';
import { LLMClient } from '../llm/openai-client';

export class KnowledgeBase {
  private rag: RAGPipeline;

  constructor() {
    const vectorStore = new VectorStore();
    const llm = new LLMClient(process.env.OPENAI_API_KEY!);
    this.rag = new RAGPipeline(vectorStore, llm);
  }

  async initialize() {
    await this.rag['vectorStore'].initialize('techflow-kb');
  }

  /**
   * ä» Markdown æ–‡ä»¶åŠ è½½æ–‡æ¡£
   */
  async loadFromMarkdown(files: string[]) {
    const fs = await import('fs/promises');

    for (const file of files) {
      const content = await fs.readFile(file, 'utf-8');
      const fileName = file.split('/').pop()!;

      await this.rag.indexDocuments([{
        id: fileName.replace('.md', ''),
        text: content,
        metadata: {
          source: file,
          type: 'markdown'
        }
      }]);
    }
  }

  /**
   * ä»ç½‘é¡µåŠ è½½æ–‡æ¡£
   */
  async loadFromWeb(urls: string[]) {
    const axios = (await import('axios')).default;
    const { JSDOM } = await import('jsdom');

    for (const url of urls) {
      const response = await axios.get(url);
      const dom = new JSDOM(response.data);
      const text = dom.window.document.body.textContent || '';

      await this.rag.indexDocuments([{
        id: new URL(url).hostname,
        text: text,
        metadata: {
          source: url,
          type: 'webpage'
        }
      }]);
    }
  }

  /**
   * é—®ç­”æ¥å£
   */
  async ask(question: string, topK: number = 5) {
    return await this.rag.query(question, { topK, includeSources: true });
  }
}

// ä½¿ç”¨ç¤ºä¾‹
async function main() {
  const kb = new KnowledgeBase();
  await kb.initialize();

  // åŠ è½½æ–‡æ¡£
  await kb.loadFromMarkdown([
    './docs/react-guide.md',
    './docs/vue3-guide.md',
    './docs/typescript-best-practices.md'
  ]);

  // é—®ç­”
  const answer = await kb.ask('React Hooks æœ‰å“ªäº›æœ€ä½³å®è·µï¼Ÿ');
  console.log('Answer:', answer.answer);
  console.log('Sources:', answer.sources);
}
```

## äº”ã€å®æˆ˜é¡¹ç›®ï¼šæ™ºèƒ½ä»£ç åŠ©æ‰‹

### 5.1 é¡¹ç›®æ¦‚è¿°

æ„å»ºä¸€ä¸ª AI é©±åŠ¨çš„ä»£ç åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š

- ğŸ’¬ ä»£ç é—®ç­”
- ğŸ” ä»£ç æœç´¢
- ğŸ“ ä»£ç ç”Ÿæˆ
- ğŸ”§ ä»£ç é‡æ„å»ºè®®
- ğŸ“š æ–‡æ¡£ç”Ÿæˆ

![Code Assistant Demo](https://images.unsplash.com/photo-1516116216624-53e697fedbea?w=1200&h=600&fit=crop)

### 5.2 é¡¹ç›®ç»“æ„

```
code-assistant/
â”œâ”€â”€ frontend/                 # Next.js å‰ç«¯
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/        # èŠå¤© API
â”‚   â”‚   â”‚   â”œâ”€â”€ search/      # æœç´¢ API
â”‚   â”‚   â”‚   â””â”€â”€ generate/    # ä»£ç ç”Ÿæˆ API
â”‚   â”‚   â”œâ”€â”€ page.tsx         # ä¸»é¡µé¢
â”‚   â”‚   â””â”€â”€ layout.tsx
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatBox.tsx
â”‚   â”‚   â”œâ”€â”€ CodeEditor.tsx
â”‚   â”‚   â””â”€â”€ FileExplorer.tsx
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ llm.ts
â”‚       â””â”€â”€ rag.ts
â”œâ”€â”€ backend/                  # Python åç«¯ï¼ˆå¯é€‰ï¼‰
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ code_analysis.py
â”‚       â””â”€â”€ test_generation.py
â””â”€â”€ shared/
    â””â”€â”€ types.ts
```

### 5.3 æ ¸å¿ƒå®ç°

**Chat APIï¼š**

```typescript
// app/api/chat/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { LLMClient } from '@/lib/llm';
import { KnowledgeBase } from '@/lib/rag';

const llm = new LLMClient(process.env.OPENAI_API_KEY!);
const kb = new KnowledgeBase();

export async function POST(request: NextRequest) {
  const { message, context } = await request.json();

  try {
    // ä½¿ç”¨ RAG å¢å¼ºçš„å›ç­”
    if (context?.useKnowledgeBase) {
      const answer = await kb.ask(message, 3);
      return NextResponse.json({
        response: answer.answer,
        sources: answer.sources
      });
    }

    // æ™®é€šå¯¹è¯
    const response = await llm.chat([
      {
        role: 'system',
        content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åŠ©æ‰‹ï¼Œæ“…é•¿å¸®åŠ©å¼€å‘è€…è§£å†³é—®é¢˜ã€‚'
      },
      { role: 'user', content: message }
    ]);

    return NextResponse.json({ response });
  } catch (error) {
    console.error('Chat API Error:', error);
    return NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    );
  }
}
```

**ä»£ç ç”Ÿæˆ APIï¼š**

```typescript
// app/api/generate/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { LLMClient } from '@/lib/llm';

const llm = new LLMClient(process.env.OPENAI_API_KEY!);

export async function POST(request: NextRequest) {
  const { prompt, language, context } = await request.json();

  const systemPrompt = `
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç ç”ŸæˆåŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ç”Ÿæˆä»£ç ã€‚

    è¦æ±‚ï¼š
    1. ä»£ç å¿…é¡»è¯­æ³•æ­£ç¡®
    2. åŒ…å«å¿…è¦çš„æ³¨é‡Š
    3. éµå¾ªæœ€ä½³å®è·µ
    4. è€ƒè™‘è¾¹ç•Œæƒ…å†µ
    5. å¦‚æœæä¾›äº†ä¸Šä¸‹æ–‡ä»£ç ï¼Œä¿æŒé£æ ¼ä¸€è‡´
  `;

  const userPrompt = `
    è¯­è¨€ï¼š${language}

    ç”¨æˆ·éœ€æ±‚ï¼š${prompt}

    ${context ? `ä¸Šä¸‹æ–‡ä»£ç ï¼š\n${context}` : ''}

    è¯·ç”Ÿæˆä»£ç ï¼š
  `;

  try {
    const response = await llm.chat([
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt }
    ]);

    return NextResponse.json({ code: response });
  } catch (error) {
    console.error('Generate API Error:', error);
    return NextResponse.json(
      { error: 'Code generation failed' },
      { status: 500 }
    );
  }
}
```

**å‰ç«¯ç»„ä»¶ï¼š**

```typescript
// components/ChatBox.tsx
'use client';

import { useState, useRef, useEffect } from 'react';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  sources?: Array<{
    id: string;
    text: string;
    score: number;
  }>;
}

export function ChatBox() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [useKB, setUseKB] = useState(true);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const sendMessage = async () => {
    if (!input.trim()) return;

    const userMessage: Message = { role: 'user', content: input };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setLoading(true);

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: input,
          context: { useKnowledgeBase: useKB }
        })
      });

      const data = await response.json();

      setMessages(prev => [...prev, {
        role: 'assistant',
        content: data.response,
        sources: data.sources
      }]);
    } catch (error) {
      console.error('Error:', error);
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: 'æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚'
      }]);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="flex flex-col h-full">
      {/* Header */}
      <div className="border-b p-4 flex justify-between items-center">
        <h2 className="text-xl font-bold">æ™ºèƒ½ä»£ç åŠ©æ‰‹</h2>
        <label className="flex items-center gap-2">
          <input
            type="checkbox"
            checked={useKB}
            onChange={(e) => setUseKB(e.target.checked)}
            className="w-4 h-4"
          />
          <span className="text-sm">ä½¿ç”¨çŸ¥è¯†åº“</span>
        </label>
      </div>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.map((msg, idx) => (
          <div
            key={idx}
            className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
          >
            <div className={`max-w-[80%] rounded-lg p-4 ${
              msg.role === 'user'
                ? 'bg-blue-500 text-white'
                : 'bg-gray-100 text-gray-900'
            }`}>
              <div className="whitespace-pre-wrap">{msg.content}</div>

              {msg.sources && msg.sources.length > 0 && (
                <div className="mt-4 pt-4 border-t border-gray-300">
                  <div className="text-xs font-semibold mb-2">å‚è€ƒæ¥æºï¼š</div>
                  {msg.sources.map((source, i) => (
                    <div key={i} className="text-xs mt-1 p-2 bg-white rounded">
                      <div className="font-semibold">[{source.id}]</div>
                      <div className="opacity-80">{source.text}</div>
                    </div>
                  ))}
                </div>
              )}
            </div>
          </div>
        ))}
        {loading && (
          <div className="flex justify-start">
            <div className="bg-gray-100 rounded-lg p-4">
              <div className="flex space-x-2">
                <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" />
                <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce delay-100" />
                <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce delay-200" />
              </div>
            </div>
          </div>
        )}
        <div ref={messagesEndRef} />
      </div>

      {/* Input */}
      <div className="border-t p-4">
        <div className="flex gap-2">
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
            placeholder="è¾“å…¥ä½ çš„é—®é¢˜..."
            className="flex-1 px-4 py-2 border rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500"
            disabled={loading}
          />
          <button
            onClick={sendMessage}
            disabled={loading || !input.trim()}
            className="px-6 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 disabled:bg-gray-300 disabled:cursor-not-allowed"
          >
            å‘é€
          </button>
        </div>
      </div>
    </div>
  );
}
```

## å…­ã€éƒ¨ç½²ä¸ä¼˜åŒ–

### 6.1 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**1. ç¼“å­˜æœºåˆ¶**

```typescript
// src/cache/redis-cache.ts
import Redis from 'ioredis';

export class CacheService {
  private redis: Redis;

  constructor() {
    this.redis = new Redis(process.env.REDIS_URL!);
  }

  async get<T>(key: string): Promise<T | null> {
    const cached = await this.redis.get(key);
    return cached ? JSON.parse(cached) : null;
  }

  async set(key: string, value: any, ttl: number = 3600): Promise<void> {
    await this.redis.setex(key, ttl, JSON.stringify(value));
  }

  async invalidate(pattern: string): Promise<void> {
    const keys = await this.redis.keys(pattern);
    if (keys.length > 0) {
      await this.redis.del(...keys);
    }
  }
}

// ä½¿ç”¨ç¼“å­˜åŒ…è£… LLM è°ƒç”¨
export class CachedLLMClient extends LLMClient {
  private cache: CacheService;

  constructor(apiKey: string) {
    super(apiKey);
    this.cache = new CacheService();
  }

  async chat(messages: Array<any>, model: string = 'gpt-4'): Promise<string> {
    // ç”Ÿæˆç¼“å­˜é”®
    const cacheKey = `llm:${model}:${JSON.stringify(messages)}`;

    // å°è¯•ä»ç¼“å­˜è·å–
    const cached = await this.cache.get<string>(cacheKey);
    if (cached) {
      console.log('Cache hit!');
      return cached;
    }

    // è°ƒç”¨ API
    const response = await super.chat(messages, model);

    // ç¼“å­˜ç»“æœï¼ˆ24å°æ—¶ï¼‰
    await this.cache.set(cacheKey, response, 86400);

    return response;
  }
}
```

**2. æ‰¹å¤„ç†ä¸å¹¶å‘**

```typescript
// æ‰¹é‡å¤„ç†æ–‡æ¡£ç´¢å¼•
async function batchIndexDocuments(
  documents: Array<any>,
  batchSize: number = 10
) {
  for (let i = 0; i < documents.length; i += batchSize) {
    const batch = documents.slice(i, i + batchSize);
    await Promise.all(
      batch.map(doc => rag.indexDocuments([doc]))
    );
    console.log(`Processed batch ${Math.floor(i / batchSize) + 1}`);
  }
}
```

**3. æµå¼è¾“å‡º**

```typescript
// app/api/stream-chat/route.ts
export async function POST(request: NextRequest) {
  const { messages } = await request.json();

  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();

      try {
        for await (const chunk of llm.chatStream(messages)) {
          controller.enqueue(encoder.encode(`data: ${chunk}\n\n`));
        }

        controller.enqueue(encoder.encode('data: [DONE]\n\n'));
        controller.close();
      } catch (error) {
        controller.error(error);
      }
    }
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  });
}
```

### 6.2 å®‰å…¨æœ€ä½³å®è·µ

**1. API å¯†é’¥ç®¡ç†**

```bash
# .env.local
OPENAI_API_KEY=sk-xxx
PINECONE_API_KEY=xxx-xxx
SERPAPI_KEY=xxx

# æ°¸è¿œä¸è¦æäº¤åˆ° Gitï¼
```

```gitignore
# .gitignore
.env
.env.local
.env.*.local
```

**2. å†…å®¹è¿‡æ»¤**

```typescript
// src/safety/content-filter.ts
export class ContentFilter {
  private harmfulPatterns = [
    /password/i,
    /api[_-]?key/i,
    /secret/i,
    /token/i
  ];

  async filter(content: string): Promise<{
    safe: boolean;
    filtered?: string;
    reason?: string;
  }> {
    // æ£€æŸ¥æ•æ„Ÿä¿¡æ¯
    for (const pattern of this.harmfulPatterns) {
      if (pattern.test(content)) {
        return {
          safe: false,
          reason: 'å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯'
        };
      }
    }

    // ä½¿ç”¨ OpenAI Moderation API
    const moderation = await fetch('https://api.openai.com/v1/moderations', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
      },
      body: JSON.stringify({ input: content })
    });

    const result = await moderation.json();

    if (result.results[0].flagged) {
      return {
        safe: false,
        reason: 'å†…å®¹è¿è§„'
      };
    }

    return { safe: true };
  }
}
```

**3. é€Ÿç‡é™åˆ¶**

```typescript
// src/rate-limit/limiter.ts
import { LRUCache } from 'lru-cache';

export class RateLimiter {
  private cache: LRUCache<string, number[]>;

  constructor(private maxRequests: number = 10, private windowMs: number = 60000) {
    this.cache = new LRUCache({
      max: 500,
      ttl: windowMs
    });
  }

  check(identifier: string): boolean {
    const now = Date.now();
    const requests = this.cache.get(identifier) || [];

    // ç§»é™¤çª—å£å¤–çš„è¯·æ±‚
    const validRequests = requests.filter(time => now - time < this.windowMs);

    if (validRequests.length >= this.maxRequests) {
      return false;
    }

    validRequests.push(now);
    this.cache.set(identifier, validRequests);
    return true;
  }
}

// Next.js API ä¸­é—´ä»¶
export async function middleware(request: NextRequest) {
  const ip = request.ip || 'unknown';
  const limiter = new RateLimiter(10, 60000); // 10 requests per minute

  if (!limiter.check(ip)) {
    return new Response('Too many requests', { status: 429 });
  }

  return NextResponse.next();
}
```

### 6.3 ç›‘æ§ä¸æ—¥å¿—

```typescript
// src/monitoring/logger.ts
import pino from 'pino';

export const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  transport: {
    target: 'pino-pretty',
    options: {
      colorize: true
    }
  }
});

// ä½¿ç”¨
logger.info({ userId, query }, 'Chat request received');
logger.error({ error }, 'LLM API failed');
```

```typescript
// src/monitoring/metrics.ts
export class Metrics {
  private counters: Map<string, number> = new Map();

  increment(name: string, value: number = 1) {
    const current = this.counters.get(name) || 0;
    this.counters.set(name, current + value);
  }

  getMetrics(): Record<string, number> {
    return Object.fromEntries(this.counters);
  }
}

// ä½¿ç”¨
const metrics = new Metrics();
metrics.increment('llm.requests.total');
metrics.increment('llm.tokens.used', tokenCount);
```

## ä¸ƒã€æ€»ç»“ä¸å±•æœ›

### æœ¬æ–‡å›é¡¾

æœ¬æ–‡ä»é›¶å¼€å§‹ï¼Œæ„å»ºäº†å®Œæ•´çš„ AI åº”ç”¨æ ˆï¼š

1. âœ… **LLM åŸºç¡€é›†æˆ**ï¼šæŒæ¡ OpenAI API çš„ä½¿ç”¨æ–¹æ³•
2. âœ… **Prompt Engineering**ï¼šå­¦ä¹ è®¾è®¡é«˜è´¨é‡çš„æç¤ºè¯
3. âœ… **AI Agent å¼€å‘**ï¼šæ„å»ºèƒ½å¤Ÿä½¿ç”¨å·¥å…·çš„æ™ºèƒ½ä»£ç†
4. âœ… **RAG ç³»ç»Ÿ**ï¼šå®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæå‡å›ç­”å‡†ç¡®æ€§
5. âœ… **å®æˆ˜é¡¹ç›®**ï¼šå®Œæˆæ™ºèƒ½ä»£ç åŠ©æ‰‹çš„å¼€å‘

### æŠ€æœ¯æ ˆæ€»ç»“

| å±‚çº§ | æŠ€æœ¯ | ç”¨é€” |
|------|------|------|
| **å‰ç«¯** | Next.js, React, TypeScript | ç”¨æˆ·ç•Œé¢ |
| **åç«¯** | Node.js, Express | API æœåŠ¡ |
| **LLM** | OpenAI GPT-4 | æ ¸å¿ƒæ™ºèƒ½ |
| **å‘é‡å­˜å‚¨** | Pinecone, Weaviate | è¯­ä¹‰æ£€ç´¢ |
| **æ¡†æ¶** | LangChain | AI åº”ç”¨å¼€å‘ |
| **ç¼“å­˜** | Redis | æ€§èƒ½ä¼˜åŒ– |
| **ç›‘æ§** | Pino, Prometheus | æ—¥å¿—å’ŒæŒ‡æ ‡ |

### æœªæ¥æ–¹å‘

**1. å¤šæ¨¡æ€ AI**
- å›¾åƒç†è§£ä¸ç”Ÿæˆ
- è§†é¢‘åˆ†æ
- è¯­éŸ³äº¤äº’

**2. æ›´å¼ºçš„ Agent**
- è‡ªä¸»è§„åˆ’ä¸æ‰§è¡Œ
- å¤š Agent åä½œ
- é•¿æœŸè®°å¿†

**3. è¾¹ç¼˜ AI**
- æœ¬åœ°æ¨¡å‹éƒ¨ç½²
- WebGPU åŠ é€Ÿ
- ç¦»çº¿ AI åº”ç”¨

**4. AI + Web3**
- å»ä¸­å¿ƒåŒ– AI
- æ•°æ®éšç§ä¿æŠ¤
- æ¿€åŠ±æœºåˆ¶

### å­¦ä¹ èµ„æº

- ğŸ“š [OpenAI Documentation](https://platform.openai.com/docs)
- ğŸ“š [LangChain Documentation](https://python.langchain.com/)
- ğŸ“š [Pinecone Learning Center](https://www.pinecone.io/learn/)
- ğŸ“š [Andrej Karpathy's Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)

---

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ï¼š**

1. é€‰æ‹©ä¸€ä¸ªå®é™…é¡¹ç›®éœ€æ±‚
2. ä»ç®€å•çš„ LLM èŠå¤©å¼€å§‹
3. é€æ­¥æ·»åŠ  RAG å’Œ Agent èƒ½åŠ›
4. å…³æ³¨ç”¨æˆ·ä½“éªŒå’Œæ€§èƒ½ä¼˜åŒ–
5. æŒç»­è¿­ä»£å’Œæ”¹è¿›

AI å¼€å‘æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œä¿æŒå­¦ä¹ å’Œå®éªŒæ˜¯æœ€é‡è¦çš„ã€‚ç¥ä½ åœ¨ AI åº”ç”¨å¼€å‘çš„æ—…ç¨‹ä¸­æ”¶è·æ»¡æ»¡ï¼

ğŸš€ **Happy Coding!**

---

*æœ¬æ–‡æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿åœ¨è¯„è®ºåŒºåˆ†äº«ä½ çš„å®è·µç»éªŒå’Œé—®é¢˜ï¼*

**ç›¸å…³æ–‡ç« ï¼š**
- [TypeScriptå®Œå…¨æŒ‡å—](/categories/å‰ç«¯/TypeScript/)
- [å‰ç«¯å·¥ç¨‹åŒ–å®æˆ˜](/categories/å‰ç«¯/å·¥ç¨‹åŒ–/)
- [å¾®å‰ç«¯æ¶æ„è®¾è®¡](/categories/å‰ç«¯/æ¶æ„/)
